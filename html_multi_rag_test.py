import os
import re
from bs4 import BeautifulSoup
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from dotenv import load_dotenv
from langchain.embeddings import HuggingFaceEmbeddings


# --- è¨­å®š ---
HTML_DIR = "html_files"
INDEX_DIR = "faiss_index"

# .envã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# --- 1. HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾— ---
def get_html_files(directory):
    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(".html")]

# --- 2. <article> ã¨ <h2> æŠ½å‡º ---
def extract_article_and_title(html_path):
    valid_tags = {'p', 'li', 'cite'}

    with open(html_path, encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'lxml')

        # <article> å–å¾—
        article = soup.find('article')

        # <time datetime=""> å–å¾—
        time_tag = article.find('time')
        datetime = time_tag.get('datetime')

        # <h2> ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        title_tag = article.find('h2')
        title = title_tag.get_text(strip=True) if title_tag else "ï¼ˆç„¡é¡Œï¼‰"

        # ã™ã¹ã¦ã® <section> ã‚’èµ°æŸ»
        all_text = ''
        for section in article.find_all('section'):

            # ç‰¹å®šã‚¿ã‚°ã‚’æŠœç²‹
            for tag in section.descendants:
                if tag.name in valid_tags:
                    tag_text = tag.get_text(strip=True)

                    # è¤‡æ•°è¡Œã‚’åˆ†å‰²
                    text_lines = tag_text.splitlines()

                    # å„è¡Œã‚’å‡¦ç†
                    for line_text in text_lines:

                        # è¡Œé ­ã®æ°´å¹³ã‚¿ãƒ–ã‚’å‰Šé™¤
                        line_text = re.sub(r'^\t+', '', line_text, flags=re.MULTILINE)

                        # <li> ã‚¿ã‚°ã«ä¸­é»’ã‚’è¿½åŠ 
                        if tag.name == 'li':
                            line_text = 'ãƒ»' + line_text

                        # <cite> ã‚¿ã‚°ã‚’ã‚«ãƒƒã‚³ã§ããã‚‹
                        if tag.name == 'cite':
                            line_text = 'ï¼ˆ' + line_text + 'ï¼‰'

                        all_text += line_text + "\n"

                        # <p> ã®å ´åˆã«æ”¹è¡Œã‚’è¿½åŠ 
                        if tag.name == 'p':
                            all_text += "\n"

            all_text += "\n"

        return title, datetime, all_text

# --- 3. Documentãƒªã‚¹ãƒˆåŒ–ï¼ˆmetadataã«titleã¨filenameã‚’å«ã‚€ï¼‰ ---
def build_documents_from_html(directory):
    documents = []
    for file_path in get_html_files(directory):
        title, datetime, all_text = extract_article_and_title(file_path)

        print(all_text)

        if all_text.strip():
            metadata = {
                "source": os.path.basename(file_path),
                "title": title,
                "datetime": datetime
            }
            doc = Document(page_content=all_text, metadata=metadata)
            documents.append(doc)
    return documents

# --- 4. åˆ†å‰²ãƒ»ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»FAISSä¿å­˜ ---
def build_vector_index(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    split_docs = splitter.split_documents(documents)

    model_name = "intfloat/multilingual-e5-large"  # E5ç³»ã®ä¸­ã§ã‚‚æ—¥æœ¬èªå¼·ã„ãƒ¢ãƒ‡ãƒ«
    embeddings = HuggingFaceEmbeddings(model_name=model_name)

#    embeddings = OpenAIEmbeddings(
#        model="text-embedding-3-small",
#        openai_api_key=OPENAI_API_KEY
#    )

    db = FAISS.from_documents(split_docs, embeddings)
    db.save_local(INDEX_DIR)
    return db

# --- 5. æ¤œç´¢ï¼†å‡ºå…¸ä»˜ãè¡¨ç¤º ---
def search_with_query(db, query):
    retriever = db.as_retriever()
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(openai_api_key=OPENAI_API_KEY),
        retriever=retriever,
        return_source_documents=True  # â† å‡ºå…¸ã‚’è¿”ã™ãŸã‚ã«å¿…è¦
    )
    result = qa.invoke(query)

    answer = result['result']
    sources = result['source_documents']

    if sources:
        source_info = sources[0].metadata
        source_str = f"\nğŸ“š å‡ºå…¸: {source_info.get('title', 'ç„¡é¡Œ')}ï¼ˆ{source_info.get('source', 'ä¸æ˜')}ï¼‰"
    else:
        source_str = "\nâš ï¸ å‡ºå…¸æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    return answer + source_str

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    docs = build_documents_from_html(HTML_DIR)
    if not docs:
        print("âŒ HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        exit()

    db = build_vector_index(docs)

    query = "å±±æœ¬æ…ä¸€éƒã¯æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã«ã¤ã„ã¦ã©ã®ã‚ˆã†ã«è€ƒãˆã¦ã„ã¾ã™ã‹ï¼Ÿã€€å…·ä½“çš„ã€ã‹ã¤ã€è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    answer = search_with_query(db, query)
    print("ğŸ§  å›ç­”:", answer)
